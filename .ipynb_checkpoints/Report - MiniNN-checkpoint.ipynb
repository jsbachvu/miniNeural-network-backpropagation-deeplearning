{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniNN Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 dÃ©cembre 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Result:\n",
    "Data set: MNIST\n",
    "\n",
    "activation function: \n",
    "1. Rectified Linear Uni a = max(0,z) , f' = (a[a>0]->1)\n",
    "2. softmax\n",
    "3. sigmoid\n",
    "\n",
    "with act func (1)\n",
    "train acc: 0.94378\n",
    "\n",
    "validation acc: 0.9346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Functions : forward, backward, update, etc in the file nn_ops.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and minibatch (as in miniNN.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Pour ne pas reload le kernel quand un fichier .py change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, math, time, sys\n",
    "import dataset_loader\n",
    "from nn_ops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the arguments \n",
    "args = parseArgs_ipython(arch = [100], act_func = \"relu\", batch_size = 500, eta = .01, n_epoch = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of the experiment\n",
      "----------\n",
      "Learning algorithm: Bprop\n",
      "Initial step-size: 0.01\n",
      "Network Architecture: [784 100  10]\n",
      "Number of parameters: 79510\n",
      "Minibatch size: 500\n",
      "Activation: relu\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fix the seed for the random generator\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "#############################\n",
    "### Dataset Handling\n",
    "#############################\n",
    "\n",
    "### Load the dataset\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "### Define the dataset variables\n",
    "n_training = train_set[0].shape[0]\n",
    "n_feature = train_set[0].shape[1]\n",
    "n_label = np.max(train_set[1])+1\n",
    "\n",
    "#############################\n",
    "### Neural Network parameters\n",
    "#############################\n",
    "\n",
    "### Activation function\n",
    "act_func_name = args.act_func\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + args.arch + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W,B,act_func,nb_params = initNetwork(nn_arch,act_func_name)\n",
    "\n",
    "#############################\n",
    "### Optimization parameters\n",
    "#############################\n",
    "eta = args.eta\n",
    "batch_size = args.batch_size\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = args.n_epoch \n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "printDescription(\"Bprop\", eta, nn_arch, act_func_name, batch_size, nb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Mini-batch creation\n",
    "j = 0\n",
    "batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "X_bacth = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500L, 784L)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### each fonction, with calculated values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Forward propagation\n",
    "Y,Yp = forward(act_func, W, B, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert Y[0].shape == (n_feature, batch_size)\n",
    "assert Y[-1].shape == (10, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert Yp[0].shape == (nn_arch[1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Compute the softmax\n",
    "out = softmax(Y[-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert out.shape == (nn_arch[-1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Compute the gradient at the top layer\n",
    "derror = out-one_hot_batch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert derror.shape == (nn_arch[-1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Backpropagation\n",
    "gradB = backward(derror, W, Yp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(gradB) == len(nn_arch)-1\n",
    "for gradw,dim in zip(gradB,nn_arch[1:]):\n",
    "    gradw.shape = (dim,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Update the parameters\n",
    "new_W, new_B = update(eta, batch_size, W, B, gradB, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Training accuracy\n",
    "train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "### Valid accuracy\n",
    "valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.061879999999999998, 0.062199999999999998]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_accuracy, valid_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.94538333, -0.96341583,  1.10676098, ..., -1.90762797,\n",
       "         -0.689435  , -0.59235028],\n",
       "        [-0.        , -0.        , -0.        , ...,  0.        ,\n",
       "          0.67769596,  0.        ],\n",
       "        [ 0.87447799,  0.86614993,  1.21733754, ..., -0.88101135,\n",
       "          2.53981916,  0.02773249],\n",
       "        ..., \n",
       "        [ 1.33807552,  1.34559832,  2.42376146, ...,  0.79582463,\n",
       "          1.63330237,  0.95396128],\n",
       "        [-0.43778557, -0.        , -0.        , ..., -1.35072294,\n",
       "         -1.87203602, -1.04154691],\n",
       "        [ 0.        ,  0.        ,  2.58826832, ..., -0.        ,\n",
       "         -0.        , -0.16207887]]),\n",
       " array([[  4.26236980e-036,   2.16895077e-075,   3.69791734e-025, ...,\n",
       "           4.69893403e-038,   6.07133111e-041,   1.40606081e-021],\n",
       "        [  9.45284153e-003,   1.29189419e-082,   9.99999994e-001, ...,\n",
       "          -1.00000000e+000,   1.07489194e-025,   4.94266333e-012],\n",
       "        [ -1.00000000e+000,  -1.00000000e+000,   4.41921863e-053, ...,\n",
       "           1.64371442e-073,   1.16480607e-049,   4.40770148e-052],\n",
       "        ..., \n",
       "        [  5.69486010e-049,   2.37885410e-094,   2.27146740e-054, ...,\n",
       "           2.96529147e-066,   2.84782734e-054,   4.40770148e-052],\n",
       "        [  5.69486010e-049,   6.57217452e-111,   2.27146740e-054, ...,\n",
       "           1.64371442e-073,   2.84782734e-054,   4.40770148e-052],\n",
       "        [  8.64979503e-042,   6.57575656e-064,   2.64179948e-011, ...,\n",
       "           5.60591245e-051,   1.11782524e-023,  -1.00000000e+000]])]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.28593428562 1.35754162957 0.66136 1.29441347623 0.6703 1\n",
      "1 2.55826606155 0.830530765597 0.79184 0.795602915167 0.8023 1\n",
      "2 3.94200302555 0.669324678727 0.82854 0.64738756728 0.8394 1\n",
      "3 5.25251884278 0.589809224979 0.84838 0.579424574595 0.8551 1\n",
      "4 6.5473566542 0.516451294674 0.86362 0.510958560364 0.8712 1\n",
      "5 7.85398922758 0.462679623172 0.87774 0.466872485869 0.887 1\n",
      "6 9.16595579473 0.42244895034 0.88648 0.433867281245 0.8955 1\n",
      "7 10.4571067111 0.394773948775 0.89376 0.412806593624 0.9012 1\n",
      "8 11.8250777586 0.377458855993 0.89906 0.39957180657 0.9057 1\n",
      "9 12.8604272371 0.36340051773 0.90238 0.389601071451 0.9072 1\n",
      "10 14.004516915 0.346265819114 0.90648 0.376562844359 0.9102 1\n",
      "11 15.1616312123 0.332874476969 0.90908 0.366455217577 0.9137 1\n",
      "12 16.3574489828 0.31982417272 0.91206 0.356926987966 0.9158 1\n",
      "13 17.4752676368 0.30746770139 0.91494 0.347933006451 0.9181 1\n",
      "14 18.757907725 0.297544538326 0.91736 0.341090541864 0.9187 1\n",
      "15 19.8783793191 0.288257329643 0.91938 0.334474357736 0.9201 1\n",
      "16 20.9669839485 0.279603197215 0.9217 0.328328760668 0.9221 1\n",
      "17 22.2248363388 0.271261648167 0.924 0.322019104337 0.9227 1\n",
      "18 23.3297638013 0.264364565798 0.92564 0.317412696012 0.9239 1\n",
      "19 24.5405491519 0.257838956139 0.92688 0.313036030881 0.9249 1\n",
      "20 25.6877139198 0.251998671286 0.92866 0.309210197956 0.9259 1\n",
      "21 26.7715999304 0.246489442512 0.93002 0.305295697453 0.9266 1\n",
      "22 27.8813218737 0.240481048709 0.93176 0.301274239301 0.9272 1\n",
      "23 29.0177490394 0.234466278175 0.9333 0.296784016739 0.9277 1\n",
      "24 30.0992458421 0.230674293186 0.9343 0.294507265991 0.9284 1\n",
      "25 31.2428580353 0.224367385489 0.93574 0.289479969702 0.9296 1\n",
      "26 32.3923647089 0.219757145592 0.9373 0.286499877756 0.9304 1\n",
      "27 33.6542360023 0.215808473949 0.93838 0.283706448521 0.9315 1\n",
      "28 34.8799420771 0.211300414033 0.93936 0.280364100057 0.9323 1\n",
      "29 36.0012744822 0.208001233399 0.94026 0.278558942996 0.9331 1\n",
      "30 37.219791067 0.203898513417 0.94148 0.275678058611 0.933 1\n",
      "31 38.4743113288 0.201014787202 0.94204 0.274117724486 0.9339 1\n",
      "32 39.6791870267 0.198708890941 0.9427 0.272438166992 0.9346 1\n",
      "33 40.889031688 0.195658426743 0.94378 0.270792936036 0.9346 1\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "train_plot = []\n",
    "valid_plot = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y,Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out-one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y)\n",
    "\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n",
    "\n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_accuracy) + \" \" + str(valid_loss) + \" \" + str(valid_accuracy) + \" \" + str(eta)\n",
    "\n",
    "    print(result_line)\n",
    "    \n",
    "    train_plot.append([train_loss, train_accuracy])\n",
    "    valid_plot.append([valid_loss, valid_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Result:\n",
    "Data set: MNIST\n",
    "\n",
    "activation function: Rectified Linear Uni a = max(0,z) , f' = (a[a>0]->1)\n",
    "\n",
    "train acc: 0.94378\n",
    "\n",
    "validation acc: 0.9346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.transpose(train_plot)[1],label='train_acc')\n",
    "plt.plot(np.transpose(valid_plot)[1],label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
